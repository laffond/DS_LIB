{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><u>Introduction</u></center></h1>\n",
    "\n",
    "<center><img src=\"../images/AI_logo.png\" width=300 height=300 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why this project ?</h2>\n",
    "\n",
    "This project comes from a personal and external need to <u>reference all data science tools and technics</u>. One of the main goal is also to <u>share knowledge</u> freely. Today, we can find awesome open sources tools and technics. However for a beginer it's really hard to select tools and to extract correct information from those sources.\n",
    "</\n",
    "\n",
    "I suppose that you have a minimum knowledge of data environement and minimum skills in informatics and mathematics. You will find introduction slide of what is big data & data science <a href=\"https://google.at\">here</a>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Which langage & tools ?</h2>\n",
    "\n",
    "Several scientific langage to do data analysis, the most common are : Matlab, Python, R & SAS. Matlab & SAS are not free so I used a lot R and Python. R is a good langage but it's used in data analysis only. Python is the most common langage for scientific computing. It is used in several domain: industry, medicine, research, ... Lot of tools and libraries use it and it is also use for other purposes: small software development & automatization for examples.This is why I mainly use <b>Python</b> for all data analysis (version 3.6).\n",
    "\n",
    "\n",
    "<img src=\"../images/Python_vs_R.png\" width=900 height=200 />\n",
    "<center>Google trends comparison between Python and R</center>\n",
    "\n",
    "I will use <b>Jupyter notebooks</b> inside Jupyter Lab to present the tools and data analysis. For Python library management, I use <b>Conda</b> environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>How to set up Python environement ?</h2>\n",
    "\n",
    "I recommend to install python from the <a href=\"https://www.python.org/\">official website</a>. Select Python 3.9 or higher. \n",
    "This project is based on Windows but it should be applicable to Linux also. \n",
    "\n",
    "Then use <a href=\"https://docs.python.org/3/library/venv.html\">VENV library</a> to create a dedicated environment for this project.  \n",
    "You should use one the 'requirements.txt' file in the 'envs' folder.\n",
    "\n",
    "I created the \"<b>dslib</b>\" environment using a setting that you can found in the requirement file <a href=\"../envs/requierements.txt\">here</a>. To create \"dslib venv environment, you just have to type following command in an the prompt:\n",
    "\n",
    "* <i>python -m venv C:\\Users\\alaffond\\PythonVenv\\dslib</i> --> change location for your virtual env \n",
    "* <i>C:\\Users\\alaffond\\PythonVenv\\dslib\\Scripts\\activate</i> \n",
    "* <i>python -m pip install --upgrade pip</i> \n",
    "* <i>python -m pip install -r \"C:\\Users\\alaffond\\OneDrive - Expleo\\Bureau\\DataScienceLib\\envs\\requirements.txt\"</i>\n",
    "    \n",
    "We will use and test other tools that will be describe later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Specific installation</h2>\n",
    "\n",
    "In this part I will describe how to set up specific libraries.\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "<h3>Get the installed packages versions to create a requirements.txt files</h3>\n",
    "- <i>python -m pip freeze</i>\n",
    "    \n",
    "***\n",
    "<h3>Add env to Jupyter Notebook/Lab</h3>\n",
    "\n",
    "In anaconda prompt:\n",
    "- <i>python -m ipykernel install --user --name=dslib</i>\n",
    "\n",
    "***\n",
    "  \n",
    "<h3>Plotly library in Jupyter Lab</h3>\n",
    "\n",
    "* <i>download <a href=\"https://nodejs.org/en/\">NodeJS</a></i>\n",
    "    \n",
    "* <i>jupyter labextension install jupyterlab-plotly</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TRICKS</h2>\n",
    "   \n",
    "<h3>File and folder manipulation</h3>\n",
    "\n",
    "<a href=\"https://treyhunner.com/2018/12/why-you-should-be-using-pathlib/\">Source</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folder with OS\n",
    "os.makedirs(os.path.join('../temp', 'empty_folder'), exist_ok=True)\n",
    "\n",
    "# Rename a folder \n",
    "os.rename('../temp/empty_folder', \"../temp/empty_folder_to_delete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\temp\\empty_folder_to_delete\n"
     ]
    }
   ],
   "source": [
    "# Concatenate path\n",
    "print(os.path.join(r\"..\\temp\", \"empty_folder_to_delete\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searched file in a folder:  ['.\\\\00_Introduction_and_Setup.ipynb', '.\\\\01_Data_Exploration.ipynb', '.\\\\02_Data_preparation.ipynb', '.\\\\03_Sklearn_models.ipynb', '.\\\\ScikitLearn_Partial_Fit.ipynb', '.\\\\temp_naiaids.ipynb', '.\\\\Unsupervised_Anomalies_Detection.ipynb']\n",
      "Searched file in a folder and its subfolder:  ['00_Introduction_and_Setup.ipynb', '01_Data_Exploration.ipynb', '02_Data_preparation.ipynb', '03_Sklearn_models.ipynb', 'ScikitLearn_Partial_Fit.ipynb']  ... \n"
     ]
    }
   ],
   "source": [
    "# Search file using Glob\n",
    "search_files = glob('./*.ipynb')\n",
    "print(\"Searched file in a folder: \", search_files)\n",
    "\n",
    "# Search file in all subfolder\n",
    "all_search_files = glob('**/*.ipynb', recursive=True)\n",
    "print(\"Searched file in a folder and its subfolder: \", all_search_files[:5], \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h3>Shortcut Jupyter</h3>\n",
    "\n",
    "Ctrl + Shift + '-' --> Split jupyter cells in two cells\n",
    "\n",
    "***\n",
    "\n",
    "<h3>Actuality</h3>\n",
    "\n",
    "* Jupyter notebook new functionnality to add table of contents: <a href=\"https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tree/master/src/jupyter_contrib_nbextensions/nbextensions/toc2\">Nbextensions_toc2</a>\n",
    "\n",
    "***\n",
    "\n",
    "<h3>Actuality</h3>\n",
    "\n",
    "* <a href=\"https://www.youtube.com/user/PyDataTV/videos\">PyDataTV</a>\n",
    "* <a href=\"http://wikistat.fr/\">Wikistat</a>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='#08088A'>TABLE OF CONTENTS</font></h2>\n",
    "\n",
    "* Introduction\n",
    "* 1 - Data exploration & visualisation\n",
    "* Standard Machine learning algorithm\n",
    "* Apach Arrow & parquet\n",
    "* imbalance sklearn\n",
    "* HistGradientBoostingClassifier sklearn vs lgbm vs xgboost\n",
    "* Pytorch vs tensorflow (FASTAI)\n",
    "* Pipeline\n",
    "* Model interpretability : LIME, SHAP, Integrated Gradients [139], and DeepLift [140] for model agnosticism\n",
    "* Model combination : Combo lib (sklearn)\n",
    "* GAN network : Cleverhans, FoolBox, ART, DEEPSEC, AdvBox.\n",
    "* Hyperparameter Optimization --> grid search / random search (like hyperband used in autoML / Bayesian optim / TPOT) & neural\n",
    "* Architecture search (NAS)\n",
    "* reinforcement learning & Active learning\n",
    "* Timeseries : Stumpy / \n",
    "* Visualisation : Matplotlib [150], Seaborn42, Bokeh43, and Altair\n",
    "* online model (Kafka)\n",
    "* autokeras\n",
    "* GPU vs no GPU & RAPIDS test \n",
    "* Big data : DASK / multi process learning (multiGPU: NVIDIA Collective Communications Library (NCCL))\n",
    "* BERT & GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i><b><u>Author:</u>  Anthony LAFFOND</b></i></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('zodiac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae24d9df1c7ba05c9af7228c646f73c31b7e50ac786b39da2a5fe211c53abfde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
